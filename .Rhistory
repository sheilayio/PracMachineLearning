power.t.test(mean=.01,sd=.04,power=.9,type="one.sample",alt="one.sided")
power.t.test(sd=.04,power=.9,type="one.sample",alt="one.sided")
?power.t.test
library(swirl)
swirl()
install_from_swirl("Regression Models")
library(swirl)
install_from_swirl("Regression Models")
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child~parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
rhs-lhs
lhs-rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
?est
varEst <- est(ols.slope, ols.ic)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varRes+varEst)
efit <- lm(accel ~ mag+dist,attenu)
summary(efit)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor~gpa_nor)
?sqe
?lse
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
swirl()
?weighted.mean
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(w*(x-0.1471)^2)
sum(w*(x-1.077)^2)
sum(w*(x-0.0025)^2)
sum(w*(x-0.300)^2)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
mean(x)
mean(y)
0.8*1.39
sum(x^2)
1.112/3.5373
cor(x,y)
cor(y,x)
?lm
summary(lm(y~x-1))
summary(lm(y~x))
data(mtcars)
summary(lm(mpg~weight))
str(mtcars)
summary(lm(mtcars$mpg~mtcars$wt))
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
x-mean(x)/sd(x)
(x-mean(x))/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
summary(lm(y~x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
sum((x-0.573)^2)
sum((x-0.8)^2)
sum((x-0.36)^2)
sum((x-0.44)^2)
swirl()
swirl()
library(swirl)
swirl()
1
1
cor(gpa_nor,gch_nor)
1
l_nor <- lm(gch_nor ~ gpa_nor)
q
sum(1\:6)/6
2
sum(1:6)/6
print(g2)
1
head(sh)
nh
w
medi
median(resampledMedians)
median(sh)
sam
sam <- sam
sam <- TRUE]\
sam <- TRUE
sam <- sample(fh,nh*B,replace=TRUE)
resam <- matrix(nh)
resam <- matrix(sam,B,nh)
median(nh)
median(row)
meds <- apply(resam,1,median)
median(fh)-median(meds)
sd(meds)
sd(resampledMedians)
c(.025,.975)
quantile(resampledMedians,c(.025,.975))
medds
meds
quantile(meds,c(.025,.975))
dim(InsectSprays)
str(InsectSprays)
names(InsectSprays)
range(Bdata$count)
range(Cdata)
range(Bdata$count)
range(Cdata$count)
BCcounts
group
testStat
testStat(BCcounts)
obs <- testStat(BCcounts)
test(obs)
range(obs)
obs <- testStat(BCcounts,group)
obs
Cdata$count
mean(Bdata$count)-mean(Cdata$count)
sample
sample(group)
perms <- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
perms > obs
mean(perms>obs)
testStat
testStat(DEcounts,group)
perms <- sapply(1 : 10000,function(i) testStat(DEcounts, sample(group)))
galton
fit <- lm(child ~ parent, galton)
sqrt(n-2)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
galton$parent
mu <- mean(galton$child)
TotalVariation
sTot <- sum(galton$child)
sTot <- sum((galton$child-mu)^2)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
sRes/sTot
1-sRes/sTot
summary(fit)$r.squared
cor
cor(galton$parent,galton$child)^2
swirl()
fit <- lm(child~parent, galton)
sqrt((sum((fit$residuals)^2))/(n-2))
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
?deviance
sRes <- deviance(galton$child)
sRes <- deviance(galton$child, lm(galton$child~galton$parent))
sRes <- sum(deviance(galton$child))
lm
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
(cor(galton$child,galton$parent))^2
cor(galton$parent,galton$child)^2
library(MASS)
data(shuttle)
?shuttle
str(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
str(shuttle)
head(shuttle)
tail(shuttle)
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
summary(fit)
?exp
summary(fit)$coef
summary(fit)$coef[0]
summary(fit)$coef[1]
summary(fit)$coef[2]
(summary(fit)$coef[1])/(summary(fit)$coef[2])
odds <- exp(summary(fit)$coef)
odds[1] / odds[2]
fit <- glm(newUse ~ as.factor(wind) + factor(magn) - 1, family="binomial", data=shuttle)
odds <- exp(summary(fit)$coef)
odds[1] / odds[2]
?I
fit3 <- glm((1 - newUse) ~ as.factor(wind) - 1, data=shuttle, family="binomial")
summary(fit3)
data(InsectSprays)
str(InsectSprays)
fit <- glm(count ~ spray - 1, data=InsectSprays, family="poisson")
summary(fit)
rate <- exp(coef(fit))
rate[1]/rate[2]
fit <- glm(count ~ as.factor(spray) + offset(log(count+1)), family="poisson", data=InsectSprays)
fit2 <- glm(count ~ as.factor(spray) + offset(log(10)+log(count+1)), family="poisson", data=InsectSprays)
summary(fit)
summary(fit2)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knotPoint <- c(0)
plot(y ~ x)
d1 <- c(0, 0 ,0, 0, 0, 0, 1 , 1, 1 , 1 , 1)
d2 <- c(1, 1 ,1, 1, 1, 1, 0 , 0, 0, 0 , 0)
summary(lm(y ~ d1*x))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
str(segmentationOriginal)
inTrain <- createDataPartition(Case~.,p=0.7,list=FALSE)
training <- segmentationOriginal[Case="Train",]
training <- segmentationOriginal[Case=="Train",]
training <- segmentationOriginal[segmentationOriginal$Case="Train",]
training <- segmentationOriginal[segmentationOriginal$Case=="Train",]
testing <- segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
?train
names(getModelInfo())
modFit <- train(Class~.,data=training,method="rpart")
install.packages("e1071")
modFit <- train(Class~.,data=training,method="rpart")
modFit$finalModel
plot(modFit$finalModel, uniform=T)
text(modFit$finalModel, cex=0.8)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
$fancyRpartPlot
?fancyRpartPlot
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
fancyRpartPlot(modFit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
str(olive)
inTrain <- createDataPartition(olive$Area,p=0.7,list=FALSE)
training <- olive[inTrain,]; testing <- olive[-inTrain,]
modFit <- train(Area~.,data=training,method="rpart")
predict(modFit,newdata = as.data.frame(t(colMeans(olive))))
newdata = as.data.frame(t(colMeans(olive))))
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata)
?train
names(getModelInfo())
?rpart
?rpart2
modFit <- train(Area~.,data=training,method="rpart2")
predict(modFit,newdata)
summary(olive)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
str(SAheart)
modFit <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,data=trainSA,method="glm",family="binomial")
modFit$finalModel
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(modFit, trainSA)
predictTest <- predict(modFit, testSA)
missClass(trainSA$chd,predictTrain)
missClass(testSA$chd,predictTest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit <- train(y~.,data=vowel.train,method="rf")
?varImp
order(varImp(modelRf), decreasing=T)
order(varImp(modFit), decreasing=T)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
head(vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528 11
dim(vowel.test) # 462 11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
set.seed(33833)
modFit <- train(y~.,data=vowel.train, method="rf")
order(varImp(modFit), decreasing=T)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
set.seed(33833)
modFit <- train(y~.,data=vowel.train, method="rf")
varImp(modFit)
?randomForest
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train)
order(varImp(modelRf), decreasing=T)
setwd("C:/Users/Goh/datasciencecoursera/PracMachineLearning")
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
str(trainDataFile)
str(testDataFile)
summary(trainDataFile)
summary(testDataFile)
sum(complete.cases(trainDataFile))
sum(complete.cases(testDataFile))
?complete.cases
tryTrain <- trainDataFile[, colSums(is.na(trainRaw)) == 0]
tryTrain <- trainDataFile[, colSums(is.na(trainDataFile)) == 0]
summary(tryTrain)
tryTest <- testDataFile[, colSums(is.na(testDataFile)) == 0]
classe <- tryTrain$classe
trainRemove <- grepl("^X|timestamp|window", names(tryTrain))
tryTrain <- tryTrain[, !trainRemove]
trainCleaned <- tryTrain[, sapply(tryTrain, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(tryTest))
tryTest <- tryTest[, !testRemove]
testCleaned <- tryTest[, sapply(tryTest, is.numeric)]
str(trainCleaned)
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
## To remove irrelevant variables
trainData <- trainDataFile[, colSums(is.na(trainDataFile))==0]
testData <- testDataFile[, colSums(is.na(testDataFile))==0]
trainToRemove <- grepl("^X|timestamp|window", names(trainData))
trainData <- trainData[, !trainToRemove]
testToRemove <- grepl("^X|timestamp|window", names(testData))
testData <- testData[, !testToRemove]
str(testData)
str(trainData)
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
## To remove irrelevant variables
trainData <- trainDataFile[, colSums(is.na(trainDataFile))==0]
testData <- testDataFile[, colSums(is.na(testDataFile))==0]
classe <- trainData$classe
trainToRemove <- grepl("^X|timestamp|window", names(trainData))
trainData <- trainData[, !trainToRemove]
trainData <- trainData[, sapply(trainData, is.numeric)]
trainData$classe <- classe
testToRemove <- grepl("^X|timestamp|window", names(testData))
testData <- testData[, !testToRemove]
testData <- testData[, sapply(testData, is.numeric)]
?rpart
?rpart2
?confusionMatrix
## Assuming data files have been downloaded to the working directory
## Load data files
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
## To remove irrelevant variables
trainData <- trainDataFile[, colSums(is.na(trainDataFile))==0]
classe <- trainData$classe
trainToRemove <- grepl("^X|timestamp|window", names(trainData))
trainData <- trainData[, !trainToRemove]
trainData <- trainData[, sapply(trainData, is.numeric)]
trainData$classe <- classe
testData <- testDataFile[, colSums(is.na(testDataFile))==0]
testToRemove <- grepl("^X|timestamp|window", names(testData))
testData <- testData[, !testToRemove]
testData <- testData[, sapply(testData, is.numeric)]
library(caret)
## Set seed for reproducibility
set.seed(12345)
## Split the trainData into training and testing sets
inTrain <- createDataPartition(trainData$classe, p=0.7, list=FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
inTrain <- createDataPartition(trainData$classe, p=0.7, list=FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
## Train the classification tree model
modelClassTree <- train(classe ~ ., data=training, method="rpart")
## Plot the classification tree
library(rattle)
fancyRpartPlot(modelClassTree$finalModel)
## Predict and check accuracy of model using testing set
modelClassTreePredict<- predict(modelClassTree, newdata=testing)
confusionMatrix(modelClassTreePredict,testing$classe)
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
## To remove irrelevant variables
trainData <- trainDataFile[, colSums(is.na(trainDataFile))==0]
classe <- trainData$classe
trainToRemove <- grepl("^X|timestamp|window", names(trainData))
trainData <- trainData[, !trainToRemove]
trainData <- trainData[, sapply(trainData, is.numeric)]
trainData$classe <- classe
testData <- testDataFile[, colSums(is.na(testDataFile))==0]
testToRemove <- grepl("^X|timestamp|window", names(testData))
testData <- testData[, !testToRemove]
testData <- testData[, sapply(testData, is.numeric)]
library(caret)
## Set seed for reproducibility
set.seed(12345)
## Split the trainData into training and testing sets
inTrain <- createDataPartition(trainData$classe, p=0.7, list=FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
## Train the classification tree model
modelClassTree <- train(classe ~ ., data=training, method="rpart")
## Plot the classification tree
library(rattle)
fancyRpartPlot(modelClassTree$finalModel)
## Predict and check accuracy of model using testing set
modelClassTreePredict<- predict(modelClassTree, newdata=testing)
confusionMatrix(modelClassTreePredict,testing$classe)
?trainControl
?train
?randomForest
?predict
?rpart
knit2html("PML_Course_Project.Rmd")
library(knitr)
knit2html("PML_Course_Project.Rmd")
browseURL("PML_Course_Project.html")
confustionMatrixRF <- confusionMatrix(predictRF,testing$classe)
str(confustionMatrixRF)
confustionMatrixRF$Accuracy
?postResample
trainDataFile <- read.csv("pml-training.csv")
testDataFile <- read.csv("pml-testing.csv")
## To remove irrelevant variables
trainData <- trainDataFile[, colSums(is.na(trainDataFile))==0]
classe <- trainData$classe
trainToRemove <- grepl("^X|timestamp|window", names(trainData))
trainData <- trainData[, !trainToRemove]
trainData <- trainData[, sapply(trainData, is.numeric)]
trainData$classe <- classe
testData <- testDataFile[, colSums(is.na(testDataFile))==0]
testToRemove <- grepl("^X|timestamp|window", names(testData))
testData <- testData[, !testToRemove]
testData <- testData[, sapply(testData, is.numeric)]
library(caret)
## Set seed for reproducibility
set.seed(12345)
## Split the trainData into training and testing sets
inTrain <- createDataPartition(trainData$classe, p=0.7, list=FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
modelCT <- rpart(classe~., data=training, method="class")
predictCT <- predict(modelCT, newdata=testing)
confusionMatrix(predictCT,testing$classe)
predictCT <- predict(modelCT, newdata=testing, method="class")
confusionMatrix(predictCT,testing$classe)
predictCT <- predict(modelCT, newdata=testing, type="class")
confusionMatrix(predictCT,testing$classe)
library(knitr)
knit2html("PML_Course_Project.Rmd")
browseURL("PML_Course_Project.html")
knit2html("PML_Course_Project.Rmd")
browseURL("PML_Course_Project.html")
?prp
prp(modelCT)
## Train the model using classification tree algorithm
modelCT <- rpart(classe~., data=training, method="class")
## Plot classification tree
prp(modelCT)
library(rplot)
library(rpart.plot)
prp(modelCT)
knit2html("PML_Course_Project.Rmd")
knit2html("PML_Course_Project.Rmd")
browseURL("PML_Course_Project.html")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
answers <- predict(modelRF, newdata=testData)
pml_write_files(answers)
knit2html("PML_Course_Project.Rmd")
browseURL("PML_Course_Project.html")
